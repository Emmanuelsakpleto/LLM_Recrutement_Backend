# -*- coding: utf-8 -*-
import os
import time
from dotenv import load_dotenv
import matplotlib.pyplot as plt
import numpy as np
import google.generativeai as genai
import logging
import torch
import re
import json
from sentence_transformers import SentenceTransformer
import gc

# Configuration des logs
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Chargement des variables d'environnement
load_dotenv()
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

if not GEMINI_API_KEY:
    raise ValueError("La cl√© API Gemini n'est pas d√©finie dans le fichier .env")

# Variable globale pour le mod√®le Sentence Transformer
_model_instance = None

def get_sentence_transformer():
    global _model_instance
    if _model_instance is None:
        # Lib√©rer la m√©moire cache CUDA si possible
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Forcer le garbage collector
        gc.collect()
        
        # Charger le mod√®le avec des options d'optimisation m√©moire
        _model_instance = SentenceTransformer(
            'distiluse-base-multilingual-cased-v1',  # Mod√®le plus l√©ger
            device='cpu'  # Forcer l'utilisation du CPU
        )
    return _model_instance

try:
    # Configuration de l'API Gemini
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-pro')
    logger.info("API Gemini configur√©e avec succ√®s")
except Exception as e:
    logger.error(f"Erreur lors de la configuration de l'API Gemini: {str(e)}")
    raise

def generate_job_description(data):
    try:
        logger.info("üöÄ D√©but de la g√©n√©ration de description")
        logger.info(f"üìù Donn√©es re√ßues: {data}")
        
        if not GEMINI_API_KEY:
            logger.error("‚ùå Cl√© API Gemini manquante")
            raise ValueError("Configuration API manquante")

        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-pro')
        logger.info("‚úÖ API Gemini configur√©e")

        prompt = f"""
        Cr√©ez une description de poste professionnelle pour :
        - Poste : {data['title']}
        - Comp√©tences : {data['skills']}
        - Exp√©rience : {data['experience']} ans
        - Contexte : {data['description']}
        """
        
        logger.info("üì§ Envoi de la requ√™te √† Gemini")
        response = model.generate_content(prompt)
        
        if response and response.text:
            logger.info("‚úÖ Description g√©n√©r√©e avec succ√®s")
            return response.text
        else:
            logger.error("‚ùå Aucune r√©ponse de l'API")
            raise ValueError("G√©n√©ration √©chou√©e")
            
    except Exception as e:
        logger.error(f"‚ùå Erreur: {str(e)}")
        raise

# Fonctions utilitaires
def cosine_similarity(a: torch.Tensor, b: torch.Tensor) -> float:
    return torch.nn.functional.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0), dim=1).item()

# D√©sactiver les avertissements de symlinks pour Hugging Face
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "true"

# Charger la cl√© API depuis .env
load_dotenv()
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if not GEMINI_API_KEY:
    raise ValueError("Cl√© API Gemini manquante. V√©rifiez le fichier .env.")

# Configurer Gemini
genai.configure(api_key=GEMINI_API_KEY)

# Utiliser la fonction get_sentence_transformer au lieu d'une instance globale
def get_embeddings(text):
    model = get_sentence_transformer()
    return model.encode(text)

print("Configuration initiale termin√©e avec succ√®s !")

def generate_job_description(brief, model="gemini-1.5-flash"):
    try:
        gen_model = genai.GenerativeModel(model)
        prompt = f"""
        G√©n√©rez une fiche de poste structur√©e au format JSON √† partir du brief suivant : "{brief}". La fiche doit contenir :
        - "title" : Titre du poste
        - "description" : Description g√©n√©rale (100-150 mots, ton professionnel)
        - "skills" : Liste des comp√©tences requises
        - "responsibilities" : Liste des responsabilit√©s principales
        - "qualifications" : Liste des qualifications (exp√©rience, formation, etc.)
        - "required_experience_years" : Nombre d'ann√©es d'exp√©rience requises (nombre)
        - "required_degree" : Dipl√¥me requis (ex. "Bachelor", "Master")
        Retournez EXCLUSIVEMENT un seul objet JSON valide, sans texte explicatif, sans balises ```json, sans r√©p√©tition.
        """
        response = gen_model.generate_content(prompt)
        text = response.text.strip()
        json_match = re.search(r'\{[\s\S]*?\}(?=\s*\{|$)', text)
        if json_match:
            return json.loads(json_match.group(0))
        else:
            print("Erreur : Aucun JSON valide trouv√© dans la r√©ponse")
            return None
    except Exception as e:
        print(f"Erreur lors de la g√©n√©ration de la fiche de poste : {str(e)}")
        return None

def extract_text_from_pdf(pdf_path):
    try:
        import pdfplumber
        with pdfplumber.open(pdf_path) as pdf:
            text = ""
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + " "
        text = re.sub(r'\s+', ' ', text.strip())
        return text
    except Exception as e:
        return f"Erreur lors de l'extraction du PDF : {str(e)}"

def analyze_cv(cv_text, model="gemini-1.5-flash"):
    try:
        gen_model = genai.GenerativeModel(model)
        prompt = f"""
        Analyse le CV suivant et extrais les informations cl√©s sous forme de JSON structur√© :
        - Comp√©tences : liste de cha√Ænes (ex. ["Python", "Java"])
        - Exp√©riences professionnelles : liste d'objets avec poste, entreprise, dur√©e, description
        - Formations : liste d'objets avec dipl√¥me, institution, ann√©e
        CV : {cv_text}
        Retourne UNIQUEMENT un JSON valide, sans texte suppl√©mentaire, sans balises markdown.
        """
        response = gen_model.generate_content(prompt)
        cleaned_response = re.sub(r'^```json\n|```$', '', response.text, flags=re.MULTILINE).strip()
        return json.loads(cleaned_response)
    except json.JSONDecodeError as e:
        return {"error": f"Impossible de parser la r√©ponse en JSON : {str(e)}", "raw_response": response.text}
    except Exception as e:
        return {"error": f"Erreur lors de l'analyse avec Gemini : {str(e)}"}

def calculate_cv_score(cv_data, job_description):
    try:
        skills_score = 0.0
        experience_score = 0.0
        education_score = 0.0       
        cv_skills = cv_data.get("Comp√©tences", [])
        job_skills = job_description.get("skills", [])
        
        if cv_skills and job_skills:
            # Utiliser get_embeddings avec conversion en tenseur PyTorch
            cv_embeddings = torch.tensor(get_embeddings(cv_skills))
            job_embeddings = torch.tensor(get_embeddings(job_skills))
            
            # Calculer les similarit√©s individuelles
            similarities = []
            for cv_emb in cv_embeddings:
                skill_similarities = []
                for job_emb in job_embeddings:
                    similarity = cosine_similarity(cv_emb, job_emb)
                    skill_similarities.append(similarity)
                similarities.append(max(skill_similarities))
            
            # Calculer le score moyen
            skills_score = sum(similarities) / len(similarities) if similarities else 0.0

        cv_experiences = cv_data.get("Exp√©riences professionnelles", [])
        required_years = job_description.get("required_experience_years", 0)
        if cv_experiences:
            total_years = 0
            for exp in cv_experiences:
                duration = exp.get("dur√©e", "").lower()
                if "ans" in duration or "an" in duration:
                    match = re.search(r'(\d+\.?\d*)', duration)
                    if match:
                        total_years += float(match.group(1))
                elif "mois" in duration:
                    match = re.search(r'(\d+\.?\d*)', duration)
                    if match:
                        total_years += float(match.group(1)) / 12
                elif "-" in duration:
                    months = 2
                    total_years += months / 12
            experience_score = min(total_years / required_years, 1.0) if required_years > 0 else 0.0

        cv_educations = cv_data.get("Formations", [])
        required_degree = job_description.get("required_degree", "")
        if cv_educations and required_degree:
            degree_levels = {"Bac": 1, "Licence": 2, "Bachelor": 2, "Master": 3, "Doctorat": 4}
            max_cv_degree_level = 0
            for edu in cv_educations:
                degree = edu.get("dipl√¥me", "")
                for deg, level in degree_levels.items():
                    if deg.lower() in degree.lower():
                        max_cv_degree_level = max(max_cv_degree_level, level)
            required_level = degree_levels.get(required_degree, 1)
            education_score = min(max_cv_degree_level / required_level, 1.0) if required_level > 0 else 0.0

        final_score = (0.5 * skills_score + 0.3 * experience_score + 0.2 * education_score) * 100

        return {
            "skills_score": skills_score * 100,
            "experience_score": experience_score * 100,
            "education_score": education_score * 100,
            "final_score": final_score
        }
    except Exception as e:
        return {"error": f"Erreur lors du calcul du score : {str(e)}"}

def visualize_scores(score_result):
    if "error" in score_result:
        print("Visualisation impossible : scores non calcul√©s.")
        return

    labels = ["Comp√©tences", "Exp√©rience", "Formation", "Final"]
    scores = [
        score_result["skills_score"],
        score_result["experience_score"],
        score_result["education_score"],
        score_result["final_score"]
    ]

    plt.figure(figsize=(8, 6))
    plt.bar(labels, scores, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])
    plt.ylim(0, 100)
    plt.title("√âvaluation du CV par rapport √† la fiche de poste", fontsize=14)
    plt.ylabel("Score (%)", fontsize=12)
    for i, score in enumerate(scores):
        plt.text(i, score + 2, f"{score:.1f}%", ha="center", fontsize=10)
    plt.tight_layout()
    plt.savefig("cv_scores.png")  # Sauvegarde pour le frontend
    plt.close()

def generate_final_report(cv_text, cv_data, score_result, job_description):
    try:
        skills_count = len(cv_data.get("Comp√©tences", []))
        exp_count = len(cv_data.get("Exp√©riences professionnelles", []))
        edu_count = len(cv_data.get("Formations", []))
        summary = (
            f"### R√©sum√© de l'√âvaluation du CV\n\n"
            f"- **Poste vis√©** : {job_description.get('title', 'Non sp√©cifi√©')}\n"
            f"- **Comp√©tences** : {skills_count} comp√©tences identifi√©es ({', '.join(cv_data.get('Comp√©tences', [])[:3]) + '...' if skills_count > 3 else ', '.join(cv_data.get('Comp√©tences', []))}). Score : {score_result.get('skills_score', 0):.1f}%\n"
            f"- **Exp√©riences** : {exp_count} exp√©riences, principalement dans {cv_data.get('Exp√©riences professionnelles', [{}])[0].get('description', 'divers domaines')[:50]}... Score : {score_result.get('experience_score', 0):.1f}%\n"
            f"- **Formations** : {edu_count} formations, incluant {cv_data.get('Formations', [{}])[0].get('dipl√¥me', 'aucun dipl√¥me')} ({cv_data.get('Formations', [{}])[0].get('institution', '')}). Score : {score_result.get('education_score', 0):.1f}%\n"
            f"- **Score final** : {score_result.get('final_score', 0):.1f}%\n\n"
            f"**Conclusion** : Le candidat pr√©sente {'une excellente' if score_result.get('final_score', 0) > 80 else 'une bonne' if score_result.get('final_score', 0) > 60 else 'une certaine'} ad√©quation avec le poste, avec des comp√©tences techniques solides et une formation pertinente. {'Continuer l‚Äô√©valuation pour un entretien.' if score_result.get('final_score', 0) > 60 else 'Consid√©rer pour des postes similaires avec moins d‚Äôexp√©rience.'}"
        )

        final_output = {
            "cv_text": cv_text[:500] + "..." if len(cv_text) > 500 else cv_text,
            "extracted_cv_data": cv_data,
            "job_description": job_description,
            "scores": score_result,
            "summary": summary
        }

        with open("recruitment_report.json", "w", encoding="utf-8") as f:
            json.dump(final_output, f, indent=2, ensure_ascii=False)

        return final_output
    except Exception as e:
        return {"error": f"Erreur lors de la g√©n√©ration du rapport : {str(e)}"}

def generate_questions_for_category(prompt, category, model="gemini-1.5-flash", max_attempts=3):
    gen_model = genai.GenerativeModel(model)
    for attempt in range(max_attempts):
        try:
            response = gen_model.generate_content(prompt)
            raw_response = response.text.strip()
            with open(f"debug_response_{category}.txt", "w", encoding="utf-8") as f:
                f.write(raw_response)
            print(f"R√©ponse brute pour {category} (tentative {attempt + 1}): {raw_response[:500]}...")

            json_match = re.search(r'\{[\s\S]*\}', raw_response)
            if not json_match:
                print(f"Tentative {attempt + 1} ({category}): Aucun JSON valide.")
                if attempt < max_attempts - 1:
                    time.sleep(2)
                    continue
                return None

            questions_data = json.loads(json_match.group(0))
            questions = questions_data.get("questions", [])
            if len(questions) != 5:
                print(f"Tentative {attempt + 1} ({category}): {len(questions)} questions re√ßues.")
                if attempt < max_attempts - 1:
                    time.sleep(2)
                    continue
                return None

            return questions
        except json.JSONDecodeError as e:
            print(f"Tentative {attempt + 1} ({category}): Erreur JSON : {str(e)}")
            if attempt < max_attempts - 1:
                time.sleep(2)
                continue
            return None
        except Exception as e:
            print(f"Tentative {attempt + 1} ({category}): Erreur : {str(e)}")
            if attempt < max_attempts - 1:
                time.sleep(2)
                continue
            return None
    return None

def generate_interview_questions(job_description, cv_data, score_result, model="gemini-1.5-flash"):
    try:
        if not job_description or "error" in cv_data or "error" in score_result:
            return {"error": "Donn√©es manquantes ou invalides."}

        job_title = job_description.get("title", "D√©veloppeur Python")
        job_skills = ", ".join(job_description.get("skills", []))
        required_years = job_description.get("required_experience_years", 3)
        cv_skills = ", ".join(cv_data.get("Comp√©tences", []))
        cv_education = cv_data.get("Formations", [{}])[0].get("dipl√¥me", "Non sp√©cifi√©")
        experience_score = score_result.get("experience_score", 0)

        company_context = "TechNova est une startup SaaS ax√©e sur l‚Äôinnovation, la collaboration, la transparence et l‚Äôimpact client."

        prompts = {
            "Job_Description": f"""
            G√©n√©rez 5 questions d'entretien pour un poste de {job_title} (comp√©tences: {job_skills}, {required_years} ans d'exp√©rience).
            Retournez un JSON avec "questions": liste de 5 objets (category: "Job Description", question, purpose).
            JSON valide uniquement.
            """,
            "Company_Culture": f"""
            G√©n√©rez 5 questions d'entretien sur la culture d'entreprise chez TechNova ({company_context}).
            Retournez un JSON avec "questions": liste de 5 objets (category: "Company Culture", question, purpose).
            JSON valide uniquement.
            """,
            "CV_Professional_Life": f"""
            G√©n√©rez 5 questions d'entretien bas√©es sur le CV (comp√©tences: {cv_skills}, formation: {cv_education}, score d'exp√©rience: {experience_score:.1f}%).
            Retournez un JSON avec "questions": liste de 5 objets (category: "CV/Professional Life", question, purpose).
            JSON valide uniquement.
            """
        }

        all_questions = []
        for category in prompts:
            questions = generate_questions_for_category(prompts[category], category.replace("/", "_"), model)
            if questions is None:
                return {"error": f"√âchec de la g√©n√©ration des questions pour {category}."}
            all_questions.extend(questions)

        if len(all_questions) != 15:
            return {"error": f"Nombre incorrect de questions g√©n√©r√©es: {len(all_questions)}."}

        questions_data = {"questions": all_questions}

        with open("interview_questions.json", "w", encoding="utf-8") as f:
            json.dump(questions_data, f, indent=2, ensure_ascii=False)

        return questions_data
    except Exception as e:
        return {"error": f"Erreur g√©n√©rale : {str(e)}"}

def collect_rh_appreciations(questions_data):
    valid_appreciations = {
        "tr√®s insatisfait": 0,
        "insatisfait": 25,
        "satisfait": 75,
        "tr√®s satisfait": 100
    }
    appreciations = []

    print("Entrez une appr√©ciation pour chaque question (tr√®s insatisfait, insatisfait, satisfait, tr√®s satisfait).")
    for q in questions_data["questions"]:
        while True:
            print(f"\nQuestion ({q['category']}) : {q['question']}")
            appreciation = input("Appr√©ciation : ").strip().lower()
            if appreciation in valid_appreciations:
                appreciations.append({
                    "question": q["question"],
                    "category": q["category"],
                    "appreciation": appreciation,
                    "score": valid_appreciations[appreciation]
                })
                break
            print("Entr√©e invalide. Options : tr√®s insatisfait, insatisfait, satisfait, tr√®s satisfait.")

    return appreciations

def generate_predictive_analysis(job_description, cv_data, score_result, questions_data, model="gemini-1.5-flash", max_attempts=3):
    try:
        if not job_description or "error" in cv_data or "error" in score_result or not questions_data:
            return {"error": "Donn√©es manquantes ou invalides."}

        gen_model = genai.GenerativeModel(model)

        appreciations = collect_rh_appreciations(questions_data)

        interview_score = [a["score"] for a in appreciations]
        interview_avg = sum(interview_score) / len(interview_score)
        culture_scores = [a["score"] for a in appreciations if a["category"] == "Company Culture"]
        culture_avg = sum(culture_scores) / len(culture_scores) if culture_scores else 50.0

        job_title = job_description.get("title", "D√©veloppeur Python")
        job_skills = ", ".join(job_description.get("skills", []))
        required_years = job_description.get("required_experience_years", 3)
        cv_skills = ", ".join(cv_data.get("Comp√©tences", []))
        cv_education = cv_data.get("Formations", [{}])[0].get("dipl√¥me", "Non sp√©cifi√©")
        scores = score_result
        company_context = "TechNova valorise l‚Äôinnovation, la collaboration, la transparence, et l‚Äôimpact client."

        prompt = f"""
        Effectuez une analyse des risques et recommandations d‚Äôonboarding pour un candidat au poste de {job_title}.
        Donn√©es :
        - Fiche de poste : Comp√©tences ({job_skills}), {required_years} ans d‚Äôexp√©rience.
        - CV : Comp√©tences ({cv_skills}), Formation ({cv_education}).
        - Scores : Comp√©tences ({scores.get('skills_score', 0):.1f}%), Exp√©rience ({scores.get('experience_score', 0):.1f}%), Formation ({scores.get('education_score', 0):.1f}%).
        - Note d‚Äôentretien : {interview_avg:.1f}% (bas√©e sur appr√©ciations RH).
        Retournez un JSON avec :
        - "risks" (liste de 2-3 risques cl√©s).
        - "recommendations" (liste de 3-5 recommandations d‚Äôonboarding : formations, mentoring, objectifs 30/60/90 jours).
        JSON valide uniquement.
        """

        for attempt in range(max_attempts):
            try:
                response = gen_model.generate_content(prompt)
                raw_response = response.text.strip()
                with open("debug_analysis_response.txt", "w", encoding="utf-8") as f:
                    f.write(raw_response)
                print(f"R√©ponse brute de l‚ÄôAPI (tentative {attempt + 1}) : {raw_response[:500]}...")

                json_match = re.search(r'\{[\s\S]*\}', raw_response)
                if not json_match:
                    print(f"Tentative {attempt + 1} : Aucun JSON valide.")
                    if attempt < max_attempts - 1:
                        time.sleep(2)
                        continue
                    return {"error": "Aucun JSON valide apr√®s plusieurs tentatives."}

                analysis = json.loads(json_match.group(0))
                if not all(key in analysis for key in ["risks", "recommendations"]):
                    print(f"Tentative {attempt + 1} : JSON incomplet.")
                    if attempt < max_attempts - 1:
                        time.sleep(2)
                        continue
                    return {"error": "JSON incomplet apr√®s plusieurs tentatives."}

                break
            except json.JSONDecodeError as e:
                print(f"Tentative {attempt + 1} : Erreur JSON : {str(e)}")
                if attempt < max_attempts - 1:
                    time.sleep(2)
                    continue
                return {"error": f"Erreur JSON apr√®s {max_attempts} tentatives : {str(e)}"}
            except Exception as e:
                print(f"Tentative {attempt + 1} : Erreur : {str(e)}")
                if attempt < max_attempts - 1:
                    time.sleep(2)
                    continue
                return {"error": f"Erreur API apr√®s {max_attempts} tentatives : {str(e)}"}

        predictive_score = (
            0.30 * scores.get("skills_score", 0) +
            0.25 * scores.get("experience_score", 0) +
            0.25 * interview_avg +
            0.10 * scores.get("education_score", 0) +
            0.10 * culture_avg
        )

        radar_data = {
            "Comp√©tences": scores.get("skills_score", 0),
            "Exp√©rience": scores.get("experience_score", 0),
            "Formation": scores.get("education_score", 0),
            "Culture": culture_avg,
            "Entretien": interview_avg
        }
        radar_labels = list(radar_data.keys())
        radar_values = list(radar_data.values()) + [radar_data["Comp√©tences"]]
        ideal_values = [100] * len(radar_labels) + [100]
        angles = np.linspace(0, 2 * np.pi, len(radar_labels), endpoint=False).tolist()
        angles += angles[:1]
        plt.figure(figsize=(8, 8))
        ax = plt.subplot(111, polar=True)
        ax.fill(angles, ideal_values, color='lightgray', alpha=0.3, label='Profil id√©al')
        ax.fill(angles, radar_values, color='skyblue', alpha=0.5, label='Candidat')
        ax.plot(angles, radar_values, color='blue', linewidth=2)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(radar_labels)
        ax.set_title("Comparaison Candidat vs Profil Id√©al", size=14, y=1.08)
        plt.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))
        plt.savefig("predictive_radar.png")
        plt.close()

        report = {
            "predictive_score": predictive_score,
            "appreciations": appreciations,
            "risks": analysis["risks"],
            "recommendations": analysis["recommendations"],
            "radar_data": radar_data
        }

        with open("predictive_performance_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        return report
    except Exception as e:
        return {"error": f"Erreur g√©n√©rale : {str(e)}"}

def cleanup_memory():
    """Nettoie la m√©moire en lib√©rant les ressources non utilis√©es."""
    global _model_instance
    
    # Lib√©rer le mod√®le s'il existe
    if _model_instance is not None:
        del _model_instance
        _model_instance = None
    
    # Lib√©rer la m√©moire CUDA si disponible
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # Forcer le garbage collector
    gc.collect()